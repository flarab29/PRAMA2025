{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Other imports\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_name = \"train_data_with_sin_ratios_with_m2_price_scaled\" # nom du fichier csv à utiliser pour train\n",
    "pred_name = \"test_data\"\n",
    "\n",
    "train_df = pd.read_csv('CSV DATA/'+ train_name +'.csv')  # fichier csv à utiliser pour train\n",
    "pred_df = pd.read_csv('CSV DATA/'+ pred_name +'.csv')  # fichier csv pour prédire\n",
    "\n",
    "target_columns = \"'prix'\" # utiliser en mode \"'prix', 'prix_m2', 'coubeh'\"\n",
    "\n",
    "#little printy print to see the data\n",
    "print(\"Train shape: \", train_df.shape)\n",
    "print(\"Prediction file shape: \", pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nb_chambres</th>\n",
       "      <th>nb_sdb</th>\n",
       "      <th>m2_interieur</th>\n",
       "      <th>m2_jardin</th>\n",
       "      <th>m2_etage</th>\n",
       "      <th>m2_soussol</th>\n",
       "      <th>nb_etages</th>\n",
       "      <th>vue_mer</th>\n",
       "      <th>vue_note</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_tres_eleve</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>jardin</th>\n",
       "      <th>etage</th>\n",
       "      <th>soussol</th>\n",
       "      <th>m2_outside</th>\n",
       "      <th>m2_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600072</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>206.243032</td>\n",
       "      <td>493.311037</td>\n",
       "      <td>108.695652</td>\n",
       "      <td>97.547380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.989992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>493.311037</td>\n",
       "      <td>699.554069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6200017</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>124.489038</td>\n",
       "      <td>1982.162765</td>\n",
       "      <td>124.489038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1982.162765</td>\n",
       "      <td>2106.651802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7600136</td>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>104.979562</td>\n",
       "      <td>106.651802</td>\n",
       "      <td>74.321813</td>\n",
       "      <td>30.657748</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0.753902</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106.651802</td>\n",
       "      <td>211.631364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11200400</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>177.443330</td>\n",
       "      <td>416.945373</td>\n",
       "      <td>177.443330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.911130</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>416.945373</td>\n",
       "      <td>594.388703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11500890</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>290.784095</td>\n",
       "      <td>812.894835</td>\n",
       "      <td>290.784095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.989992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>812.894835</td>\n",
       "      <td>1103.678930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  nb_chambres  nb_sdb  m2_interieur    m2_jardin    m2_etage  \\\n",
       "0   3600072            4    2.75    206.243032   493.311037  108.695652   \n",
       "1   6200017            3    1.00    124.489038  1982.162765  124.489038   \n",
       "2   7600136            2    2.00    104.979562   106.651802   74.321813   \n",
       "3  11200400            3    2.50    177.443330   416.945373  177.443330   \n",
       "4  11500890            3    2.50    290.784095   812.894835  290.784095   \n",
       "\n",
       "   m2_soussol  nb_etages  vue_mer  vue_note  ...  cluster_tres_eleve  Year  \\\n",
       "0   97.547380        1.0        0         0  ...                   0  2015   \n",
       "1    0.000000        1.5        0         0  ...                   0  2014   \n",
       "2   30.657748        2.0        0         0  ...                   0  2014   \n",
       "3    0.000000        2.0        0         0  ...                   0  2014   \n",
       "4    0.000000        2.0        0         0  ...                   0  2015   \n",
       "\n",
       "   Month  Day  cos_month  jardin  etage  soussol   m2_outside     m2_total  \n",
       "0      3   30  -0.989992       1      1        1   493.311037   699.554069  \n",
       "1     11   12   0.004426       1      1        0  1982.162765  2106.651802  \n",
       "2      7   18   0.753902       1      1        1   106.651802   211.631364  \n",
       "3      9   23  -0.911130       1      1        0   416.945373   594.388703  \n",
       "4      3   12  -0.989992       1      1        0   812.894835  1103.678930  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(columns=[target_columns]), train_df[target_columns], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.drop(columns=['Year', 'Month', 'Day'], inplace=True)\n",
    "test_new.drop(columns=['Year', 'Month', 'Day'], inplace=True)\n",
    "\n",
    "clusters = ['cluster_tres_bas', 'cluster_bas', 'cluster_moyen', 'cluster_eleve', 'cluster_tres_eleve']\n",
    "train_new.drop(columns=clusters, inplace=True)\n",
    "test_new.drop(columns=clusters, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'prix', 'nb_chambres', 'nb_sdb', 'm2_interieur', 'm2_jardin',\n",
       "       'm2_etage', 'm2_soussol', 'nb_etages', 'vue_mer', 'vue_note',\n",
       "       'etat_note', 'design_note', 'annee_construction', 'annee_renovation',\n",
       "       'm2_interieur_15voisins', 'm2_jardin_15voisins', 'zipcode', 'cos_month',\n",
       "       'jardin', 'etage', 'soussol', 'm2_outside', 'm2_total',\n",
       "       'prix_m2_interieur', 'prix_m2_total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'nb_chambres', 'nb_sdb', 'm2_interieur', 'm2_jardin', 'm2_etage',\n",
       "       'm2_soussol', 'nb_etages', 'vue_mer', 'vue_note', 'etat_note',\n",
       "       'design_note', 'annee_construction', 'annee_renovation',\n",
       "       'm2_interieur_15voisins', 'm2_jardin_15voisins', 'zipcode', 'cos_month',\n",
       "       'jardin', 'etage', 'soussol', 'm2_outside', 'm2_total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scaling_data(df1, df2) :\n",
    "    scaled_name1 = f\"train-clusters_sinus_m2_scaled.csv\"\n",
    "    scaled_name2 = f\"test-clusters_sinus_m2_scaled.csv\"\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale the data\n",
    "    to_scale1 = df1  # to not modify the original one\n",
    "    to_scale2 = df2\n",
    "    scaled_df1 = pd.DataFrame(scaler.fit_transform(to_scale1), columns=to_scale1.columns)\n",
    "    scaled_df2 = pd.DataFrame(scaler.transform(to_scale2), columns=to_scale2.columns)\n",
    "\n",
    "    scaled_df1.to_csv(scaled_name1, index=False)\n",
    "    scaled_df2.to_csv(scaled_name2, index=False)\n",
    "    return scaler, scaled_df1, scaled_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- prix\n- prix_m2_interieur\n- prix_m2_total\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scaler, train_scaled, test_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaling_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_new\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m, in \u001b[0;36mscaling_data\u001b[1;34m(df1, df2)\u001b[0m\n\u001b[0;32m     12\u001b[0m to_scale2 \u001b[38;5;241m=\u001b[39m df2\n\u001b[0;32m     13\u001b[0m scaled_df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaler\u001b[38;5;241m.\u001b[39mfit_transform(to_scale1), columns\u001b[38;5;241m=\u001b[39mto_scale1\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m---> 14\u001b[0m scaled_df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_scale2\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mto_scale2\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     16\u001b[0m scaled_df1\u001b[38;5;241m.\u001b[39mto_csv(scaled_name1, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m scaled_df2\u001b[38;5;241m.\u001b[39mto_csv(scaled_name2, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\flobm\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\flobm\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:508\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m    Transformed data.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 508\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    517\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\flobm\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\flobm\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- prix\n- prix_m2_interieur\n- prix_m2_total\n"
     ]
    }
   ],
   "source": [
    "scaler, train_scaled, test_scaled = scaling_data(train_new, test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - regression pénalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_reg(X_train, y_train, df_test, var_cible):\n",
    "    # Initialisation du modèle Lasso\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # On cherche X_test\n",
    "    X_test = df_test[var_cible]\n",
    "\n",
    "    # Prédiction sur l'ensemble de test\n",
    "    y_pred = lasso.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First train model - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def current_time_filename():\n",
    "    # Obtenir la date et l'heure actuelles au format mois_jour_heure_min pour au cas où on veut spammer le kaggle\n",
    "    current_time = datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "\n",
    "    # Et paf le nom de fichier avec la date et l'heure actuelles\n",
    "    file_name = f\"predictions_{current_time}\"\n",
    "\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Nombre d'arbres dans la forêt\n",
    "    'max_depth': [10, 20, 30],#, None],  # Profondeur maximale des arbres\n",
    "    'min_samples_split': [2, 5, 10],  # Nombre minimal d'échantillons pour une division interne\n",
    "    'min_samples_leaf': [1, 2, 4],    # Nombre minimal d'échantillons par feuille\n",
    "    'bootstrap': [True, False]        # Utiliser bootstrap ou non, via ce qu'à dit le prof\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with price m2\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')\n",
    "\n",
    "def train_with_total_m2_price(train_df, test_df):\n",
    "    X_train = train_df.drop(columns=['id', 'prix', 'prix_m2_interieur', 'prix_m2_total'])\n",
    "    y_train = train_df[['prix_m2_interieur', 'prix_m2_total']]\n",
    "\n",
    "    print(\"X_train shape: \", X_train.shape)\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_with_m2_price(train_df, test_df):\n",
    "    X_train = train_df.drop(columns=['id', 'prix', 'prix_m2_interieur', 'prix_m2_total'])\n",
    "    y_train = train_df['prix_m2_interieur']\n",
    "\n",
    "    print(\"X_train shape: \", X_train.shape)\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_with_price(train_df, test_df):\n",
    "    X_train = train_df.drop(columns=['id', 'prix'])\n",
    "    y_train = train_df['prix']\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On cherche les meilleurs paramètres avec grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs params :\n",
    "Sans zip code, sans clustering des zip code, etude du prix\n",
    "- Meilleurs paramètres : {'bootstrap': True, 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "\n",
    "Clustering des zips, etude du prix (52min13.9s)\n",
    "- Meilleurs paramètres : {'bootstrap': True, 'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "\n",
    "Clustering, etude du prix au m2 intérieur (24min14.4s)\n",
    "- Meilleurs paramètres : {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "\n",
    "Clustering, prix m2 total, sin et scale (18min58.1s)\n",
    "- Meilleurs paramètres : {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rf_m2_price = train_with_m2_price(train_new, test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rf_full_price = train_with_price(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (17147, 27)\n",
      "y_train shape:  (17147, 2)\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Meilleurs paramètres : {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# best_rf_total_m2_price = train_with_total_m2_price(train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_m2(train_df, test_df):\n",
    "    X_train = train_df.drop(columns=['id', 'prix', 'prix_m2_interieur', 'prix_m2_total'])\n",
    "    y_train = train_df['prix_m2_interieur']\n",
    "\n",
    "    model = RandomForestRegressor(bootstrap = True, max_depth = 20, min_samples_leaf = 2, min_samples_split = 2, n_estimators = 200, verbose=2, n_jobs=-1)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "    y_pred_m2 = model.predict(X_test) # la ca predit pour un prix au m2 !\n",
    "    print(len(y_pred_m2)) # ca c'est pour du debug\n",
    "\n",
    "    date_filename = current_time_filename()\n",
    "    print(date_filename)\n",
    "\n",
    "    filename = f\"{date_filename}-cluster-prix-m2-1.csv\"\n",
    "    # Sauvegarder les résultats dans un fichier CSV avec ce nouveau nom\n",
    "    test_df['prix_m2'] = y_pred_m2\n",
    "    test_df['prix'] = test_df['prix_m2'] * test_df['m2_interieur']\n",
    "    test_df[['id', 'prix']].to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"Fichier sauvegardé sous : {filename}\")\n",
    "\n",
    "def predict_for_total_m2(train_df, test_df, estimator = False, is_scaled = False, m2_price_col = None, id_col = None):\n",
    "    X_train = train_df.drop(columns=['id', 'prix', 'prix_m2_interieur', 'prix_m2_total'])\n",
    "    y_train = train_df[['prix_m2_interieur', 'prix_m2_total']]\n",
    "    \n",
    "    if not estimator :\n",
    "        model = RandomForestRegressor(bootstrap = True, max_depth = 20, min_samples_leaf = 2, min_samples_split = 2, n_estimators = 200, verbose=1, n_jobs=-1)\n",
    "    else :\n",
    "        model = estimator\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "    y_pred_m2 = model.predict(X_test) # la ca predit pour un prix au m2 !\n",
    "    print(len(y_pred_m2)) # ca c'est pour du debug\n",
    "\n",
    "    #print(y_pred_m2[0]) # on voit que le premier de chaque duo est le prix au m2 interieur, l'autre au total\n",
    "\n",
    "    y_pred_m2_array = seperate_total_and_inside(y_pred_m2)\n",
    "\n",
    "    date_filename = current_time_filename()\n",
    "    print(date_filename)\n",
    "\n",
    "    # Sauvegarder les résultats dans un fichier CSV avec ce nouveau nom\n",
    "    test_df['prix_m2_tot'] = y_pred_m2_array[1]\n",
    "    test_df['prix'] = test_df['prix_m2_tot'] * test_df['m2_total']\n",
    "\n",
    "    if is_scaled :\n",
    "        filename = f\"{date_filename}-cluster-prix-m2tot-DeScaled.csv\"\n",
    "        # Initialize the MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        prediction = test_df['prix']\n",
    "\n",
    "        # Fit the MinMaxScaler\n",
    "        scaler.fit(m2_price_col.values.reshape(-1, 1))\n",
    "        # Reshape predictions to a 2D array (necessary for inverse_transform)\n",
    "        predictions_scaled_reshaped = np.array(prediction).reshape(-1, 1)\n",
    "        # Reverse the Min-Max scaling (inverse transform)\n",
    "        predictions_original = scaler.inverse_transform(predictions_scaled_reshaped)\n",
    "\n",
    "        test_df['prix'] = predictions_original\n",
    "        test_df['id'] = id_col\n",
    "\n",
    "        test_df[['id', 'prix']].to_csv(filename, index=False)\n",
    "    \n",
    "    else :\n",
    "        filename = f\"{date_filename}-cluster-prix-m2tot.csv\"\n",
    "        test_df[['id', 'prix']].to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"Fichier sauvegardé sous : {filename}\")\n",
    "\n",
    "    return y_pred_m2_array[1]\n",
    "\n",
    "def seperate_total_and_inside(array_to_separate):\n",
    "    array_inside = []\n",
    "    array_tot = []\n",
    "    for c in array_to_separate :\n",
    "        array_inside.append(c[0])\n",
    "        array_tot.append(c[1])\n",
    "    return array_inside, array_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_for_total_m2(train_scaled, test_scaled, best_rf_total_m2_price, True, m2_price_col = train_new['prix_m2_total']*train_new['m2_total'], id_col = test_new['id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# le code du cours de Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a regression model\n",
    "#model = RandomForestRegressor(n_estimators=100, random_state=42, verbose=1, n_jobs=-1)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=2, n_jobs=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data (we don't have 'Sales_Qty' for the test set)\n",
    "X_test = test_df.drop(columns=['index', 'Sales_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model = best_model\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions (You can store them in a new DataFrame and save to CSV)\n",
    "test_df['Predicted_Sales_Qty'] = predictions\n",
    "test_df[['index', 'Predicted_Sales_Qty']].to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, evaluate the model on the training set\n",
    "train_predictions = model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, train_predictions)\n",
    "print(f\"Mean Squared Error on training set: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
