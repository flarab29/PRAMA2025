{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f0cabb",
   "metadata": {},
   "source": [
    "# Version 1 : L'idée générale du _behemoth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# ---------- 0) DATASET TUILE PAR TUILE ----------\n",
    "data_dir   = Path(\"C:/Users/flobm/PRAMA_2025\")\n",
    "features_d = data_dir / \"train_input/train_input/moco_features\"\n",
    "labels_p   = data_dir / \"train_output.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(labels_p).set_index(\"Sample ID\")\n",
    "\n",
    "X_lst, y_lst, g_lst = [], [], []\n",
    "\n",
    "for sid, target in labels_df[\"Target\"].items():\n",
    "    path = features_d / f\"{sid}\"\n",
    "    if not path.exists():\n",
    "        print(f\"Missing {path}\")\n",
    "        continue\n",
    "    mat = np.load(path)[:, 3:]               # (n_tiles, 2048)\n",
    "    n = mat.shape[0]\n",
    "\n",
    "    X_lst.append(mat)\n",
    "    y_lst.append(np.full(n, target))\n",
    "    g_lst.append(np.repeat(str(sid), n))     # ← FIX : pas de troncature\n",
    "\n",
    "X       = np.vstack(X_lst)\n",
    "y       = np.concatenate(y_lst)\n",
    "groups  = np.concatenate(g_lst)              # (N_tiles,)\n",
    "\n",
    "# ---------- 1) TRAIN / VAL SPLIT ----------\n",
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "X_tr, y_tr = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "# ---------- 2) STAGE-1 ----------\n",
    "base = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "base.fit(X_tr, y_tr)\n",
    "\n",
    "proba_all = base.predict_proba(X)[:, 1]\n",
    "\n",
    "# ---------- 3) SELECTION TOP-k ----------\n",
    "\n",
    "print(\"début de la sélection des 200 tuiles les plus expressives\")\n",
    "k = 200\n",
    "sel_mask = np.zeros_like(y, dtype=bool)\n",
    "\n",
    "for sid in labels_df.index:                  # sid en chaîne complète\n",
    "    idx = np.where(groups == str(sid))[0]\n",
    "    if len(idx) == 0:                        # si path manquant\n",
    "        continue\n",
    "    label = y[idx[0]]\n",
    "    scores = proba_all[idx] if label == 1 else (1 - proba_all[idx])\n",
    "    top_idx = idx[np.argsort(-scores)[:min(k, len(idx))]]\n",
    "    sel_mask[top_idx] = True\n",
    "\n",
    "X_sel, y_sel = X[sel_mask], y[sel_mask]\n",
    "groups_sel   = groups[sel_mask]\n",
    "\n",
    "print(\"sélection des 200 meilleures tuiles effectuée\")\n",
    "\n",
    "# Re-utilise la même séparation groupe-wise\n",
    "train_sel = sel_mask & np.isin(np.arange(len(y)), train_idx)\n",
    "val_sel   = sel_mask & np.isin(np.arange(len(y)), val_idx)\n",
    "\n",
    "X_tr2, y_tr2 = X[train_sel], y[train_sel]\n",
    "X_val2, y_val2 = X[val_sel], y[val_sel]\n",
    "\n",
    "# ---------- 4) STAGE-2 ----------\n",
    "final = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(y_tr2 == 0).sum() / (y_tr2 == 1).sum(),\n",
    "    eval_metric=\"auc\",\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "final.fit(X_tr2, y_tr2,\n",
    "          eval_set=[(X_val2, y_val2)],\n",
    "          verbose=True)\n",
    "\n",
    "# ---------- 5) METRICS ----------\n",
    "proba_val = final.predict_proba(X_val2)[:, 1]\n",
    "auc  = roc_auc_score(y_val2, proba_val)\n",
    "aupr = average_precision_score(y_val2, proba_val)\n",
    "\n",
    "print(f\"Tiles retained  : {X_sel.shape[0]:,} / {X.shape[0]:,}\")\n",
    "print(f\"AUC  ROC        : {auc:.4f}\")\n",
    "print(f\"Average Precision: {aupr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed00648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, datetime, re\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0)  CHEMINS (à adapter si besoin)\n",
    "# ------------------------------------------------------------------\n",
    "data_dir          = Path(\"C:/Users/flobm/PRAMA_2025\")\n",
    "test_features_dir = data_dir / \"test_input/test_input/moco_features\"\n",
    "out_dir           = data_dir / \"Outputs\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1)  PARAMÈTRES INFERENCE\n",
    "# ------------------------------------------------------------------\n",
    "k_top_tiles = 200\n",
    "agg_method  = \"mean\"          # \"mean\" ou \"max\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2)  TRI “NATURAL SORT” + UTILS\n",
    "# ------------------------------------------------------------------\n",
    "def natural_key(path_obj):\n",
    "    \"\"\"Clé de tri type '10' > '2', gère aussi des préfixes/suffixes.\"\"\"\n",
    "    return [\n",
    "        int(tok) if tok.isdigit() else tok.lower()\n",
    "        for tok in re.split(r\"(\\d+)\", path_obj.name)\n",
    "    ]\n",
    "\n",
    "def sample_id_from_path(p: Path) -> str:\n",
    "    \"\"\"Récupère l'ID sans l'extension (si elle existe).\"\"\"\n",
    "    return p.stem if p.suffix else p.name\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3)  FONCTION PRINCIPALE\n",
    "# ------------------------------------------------------------------\n",
    "def predict_test_and_save(\n",
    "    base_model,\n",
    "    final_model,\n",
    "    k=k_top_tiles,\n",
    "    aggregate=agg_method,\n",
    "    features_dir=test_features_dir,\n",
    "    output_dir=out_dir,\n",
    "    prefix=\"test_output\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Prédit la proba par lame sur le jeu de test et écrit un CSV\n",
    "    'Sample ID,Target' trié par ID croissant,\n",
    "    avec .npy ajouté dans la colonne Sample ID.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "\n",
    "    # Liste des fichiers, triés “naturellement”\n",
    "    files = sorted(\n",
    "        [p for p in features_dir.iterdir() if p.is_file()],\n",
    "        key=natural_key,\n",
    "    )\n",
    "\n",
    "    for path in files:\n",
    "        sample_id_raw = sample_id_from_path(path)      # ex. \"ID_022\"\n",
    "        sample_id_npy = f\"{sample_id_raw}.npy\"         # ex. \"ID_022.npy\"\n",
    "        mat = np.load(path)[:, 3:]                     # (n_tiles, 2048)\n",
    "\n",
    "        # ----- Étape A : scores base_model -----\n",
    "        proba_base = base_model.predict_proba(mat)[:, 1]\n",
    "\n",
    "        # ----- Étape B : top-k -----\n",
    "        if k is not None and len(proba_base) > k:\n",
    "            sel_idx   = np.argsort(-proba_base)[:k]\n",
    "            feats_sel = mat[sel_idx]\n",
    "        else:\n",
    "            feats_sel = mat\n",
    "\n",
    "        # ----- Étape C : final_model -----\n",
    "        proba_tiles = final_model.predict_proba(feats_sel)[:, 1]\n",
    "\n",
    "        # ----- Étape D : agrégation -----\n",
    "        if aggregate == \"mean\":\n",
    "            sample_prob = proba_tiles.mean()\n",
    "        elif aggregate == \"max\":\n",
    "            sample_prob = proba_tiles.max()\n",
    "        else:\n",
    "            raise ValueError(\"aggregate must be 'mean' or 'max'\")\n",
    "\n",
    "        preds.append((sample_id_npy, sample_prob))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4)  EXPORT CSV\n",
    "    # ------------------------------------------------------------------\n",
    "    preds.sort(key=lambda x: natural_key(Path(x[0])))          # tri par ID\n",
    "    timestamp = datetime.datetime.now().strftime(\"%m_%d-%H_%M\")\n",
    "    out_path  = output_dir / f\"{prefix}-{timestamp}.csv\"\n",
    "\n",
    "    pd.DataFrame(preds, columns=[\"Sample ID\", \"Target\"]).to_csv(out_path, index=False)\n",
    "    print(f\"[✓] Submission sauvegardée dans : {out_path}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5)  APPEL (base et final sont tes deux modèles déjà entraînés)\n",
    "# ------------------------------------------------------------------\n",
    "predict_test_and_save(base, final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e6974",
   "metadata": {},
   "source": [
    "# Version 2 : Cross validation et méthodes pour limiter l'over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6178e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totales tuiles : 344000\n",
      "\n",
      "── Fold 1/5 ───────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:47:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → 55,000 tuiles train  | 13,800 val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:51:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC fold = 0.9757\n",
      "\n",
      "── Fold 2/5 ───────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:07:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → 55,000 tuiles train  | 13,800 val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:10:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC fold = 0.9609\n",
      "\n",
      "── Fold 3/5 ───────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:27:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → 55,000 tuiles train  | 13,800 val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:30:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC fold = 0.9572\n",
      "\n",
      "── Fold 4/5 ───────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:47:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → 55,000 tuiles train  | 13,800 val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC fold = 0.9675\n",
      "\n",
      "── Fold 5/5 ───────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:07:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → 55,200 tuiles train  | 13,600 val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flobm\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:10:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC fold = 0.9755\n",
      "\n",
      "========== RÉSUMÉ CV ==========\n",
      "AUC (OOF, top-k) = 0.6602\n",
      "Moyenne folds    = 0.9674 ± 0.0075\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0)  CHEMINS (garde tes chemins actuels)\n",
    "# ------------------------------------------------------------------\n",
    "data_dir          = Path(\"C:/Users/flobm/PRAMA_2025\")\n",
    "features_dir      = data_dir / \"train_input/train_input/moco_features\"\n",
    "labels_path       = data_dir / \"train_output.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1)  CHARGEMENT TUILE-PAR-TUILE + GROUPES\n",
    "# ------------------------------------------------------------------\n",
    "labels_df = pd.read_csv(labels_path).set_index(\"Sample ID\")\n",
    "\n",
    "X_lst, y_lst, g_lst = [], [], []          # features, labels, group-ids\n",
    "\n",
    "for sid, target in labels_df[\"Target\"].items():\n",
    "    f = features_dir / f\"{sid}\"\n",
    "    if not f.exists():\n",
    "        print(f\"Missing {f}\")\n",
    "        continue\n",
    "    mat = np.load(f)[:, 3:]               # (n_tiles, 2048)\n",
    "    n   = mat.shape[0]\n",
    "\n",
    "    X_lst.append(mat)\n",
    "    y_lst.append(np.full(n, target))\n",
    "    g_lst.append(np.repeat(str(sid), n))  # ID complet, pas tronqué\n",
    "\n",
    "X       = np.vstack(X_lst)\n",
    "y       = np.concatenate(y_lst)\n",
    "groups  = np.concatenate(g_lst)           # (N_tiles,)\n",
    "\n",
    "print(\"Totales tuiles :\", X.shape[0])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2)  PARAMÈTRES GLOBAUX\n",
    "# ------------------------------------------------------------------\n",
    "k_top     = 200                 # top-k tuiles retenues / lame\n",
    "n_folds   = 5                   # GroupKFold\n",
    "seed_base = 42\n",
    "\n",
    "xgb_params = dict(\n",
    "    n_estimators      = 150,\n",
    "    max_depth         = 5,\n",
    "    min_child_weight  = 7,\n",
    "    subsample         = 0.7,\n",
    "    colsample_bytree  = 0.7,\n",
    "    gamma             = 0.3,\n",
    "    reg_lambda        = 2,      # L2\n",
    "    booster           = \"dart\", # dropout among trees\n",
    "    rate_drop         = 0.1,\n",
    "    learning_rate     = 0.05,\n",
    "    eval_metric       = \"auc\",\n",
    "    use_label_encoder = False,\n",
    "    n_jobs            = -1,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3)  FONCTION SELECTION TOP-k  (sur un sous-ensemble)\n",
    "# ------------------------------------------------------------------\n",
    "def select_top_k(mat, probs, label, k):\n",
    "    \"\"\"Retourne indices des k tuiles 'les plus cohérentes' avec le label.\"\"\"\n",
    "    if k is None or len(probs) <= k:\n",
    "        return np.arange(len(probs))\n",
    "    score = probs if label == 1 else (1 - probs)\n",
    "    return np.argsort(-score)[:k]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4)  TRAIN EN CROSS-VAL 5 FOLDS PAR LAME\n",
    "# ------------------------------------------------------------------\n",
    "gkf           = GroupKFold(n_splits=n_folds)\n",
    "oof_pred      = np.zeros_like(y, dtype=float)\n",
    "models        = []\n",
    "fold_aucs     = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "    print(f\"\\n── Fold {fold+1}/{n_folds} ───────────────────────────\")\n",
    "\n",
    "    # -------- Stage-1  : modèle brouillon sur toutes les tuiles TRAIN --------\n",
    "    base = xgb.XGBClassifier(random_state=seed_base + fold,\n",
    "                             **{k: v for k, v in xgb_params.items()\n",
    "                                if k not in (\"booster\", \"rate_drop\")})  # booster 'gbtree'\n",
    "    base.fit(X[tr_idx], y[tr_idx])\n",
    "\n",
    "    # -------- Sélection top-k (TRAIN + VAL séparément) --------\n",
    "    sel_mask = np.zeros_like(y, dtype=bool)\n",
    "\n",
    "    for sid in np.unique(groups):\n",
    "        idx_all = np.where(groups == sid)[0]            # tuiles de cette lame\n",
    "        probs    = base.predict_proba(X[idx_all])[:, 1]\n",
    "        label    = y[idx_all[0]]\n",
    "        top_idx  = idx_all[select_top_k(X[idx_all], probs, label, k_top)]\n",
    "        sel_mask[top_idx] = True\n",
    "\n",
    "    # Indices sélectionnés par split\n",
    "    tr_sel  = sel_mask & np.isin(np.arange(len(y)), tr_idx)\n",
    "    val_sel = sel_mask & np.isin(np.arange(len(y)), val_idx)\n",
    "\n",
    "    X_tr, y_tr = X[tr_sel],  y[tr_sel]\n",
    "    X_val, y_val = X[val_sel], y[val_sel]\n",
    "\n",
    "    print(f\"  → {X_tr.shape[0]:,} tuiles train  | {X_val.shape[0]:,} val\")\n",
    "\n",
    "    # -------- Stage-2  : modèle final régularisé --------\n",
    "    final = xgb.XGBClassifier(\n",
    "        scale_pos_weight = (y_tr == 0).sum() / (y_tr == 1).sum(),\n",
    "        random_state     = seed_base + fold,\n",
    "        **xgb_params\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # tentative 1 : version xgboost ≥ 1.3  (early_stopping_rounds dispo)\n",
    "    # tentative 2 : version 1.0-1.2       (callbacks dispo)\n",
    "    # fallback    : version ≤ 0.90        (aucun des deux)\n",
    "    # ---------------------------------------------------\n",
    "    fitted = False\n",
    "    try:\n",
    "        final.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=40,\n",
    "            verbose=False,\n",
    "        )\n",
    "        fitted = True\n",
    "    except TypeError:\n",
    "        pass                        # on retente plus bas\n",
    "\n",
    "    if not fitted:\n",
    "        try:\n",
    "            from xgboost.callback import EarlyStopping       # peut ne pas exister\n",
    "            final.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False,\n",
    "                callbacks=[EarlyStopping(rounds=40, save_best=True)],\n",
    "            )\n",
    "            fitted = True\n",
    "        except (TypeError, ImportError):\n",
    "            pass\n",
    "\n",
    "    if not fitted:                  # dernier recours : pas d'early-stop\n",
    "        final.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False,\n",
    "        )\n",
    "    # -------- Prédictions fold + métrique --------\n",
    "    val_pred           = final.predict_proba(X_val)[:, 1]\n",
    "    oof_pred[val_sel]  = val_pred\n",
    "    auc_fold           = roc_auc_score(y_val, val_pred)\n",
    "    fold_aucs.append(auc_fold)\n",
    "    print(f\"  AUC fold = {auc_fold:.4f}\")\n",
    "\n",
    "    models.append(final)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5)  METRICS CV\n",
    "# ------------------------------------------------------------------\n",
    "cv_auc  = roc_auc_score(y[np.flatnonzero(sel_mask)], oof_pred[sel_mask])\n",
    "print(\"\\n========== RÉSUMÉ CV ==========\")\n",
    "print(f\"AUC (OOF, top-k) = {cv_auc:.4f}\")\n",
    "print(f\"Moyenne folds    = {np.mean(fold_aucs):.4f} ± {np.std(fold_aucs):.4f}\")\n",
    "\n",
    "# À ce stade :\n",
    "#   • `models` contient 5 modèles (un par fold) pour l’ENSEMBLE final.\n",
    "#   • Pour l’inference test : faire la moyenne des proba des 5 modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd36b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Submission sauvegardée dans : C:\\Users\\flobm\\PRAMA_2025\\Outputs\\test_output-06_18-23_36.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, datetime, re\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0)  CHEMINS (à adapter si besoin)\n",
    "# ------------------------------------------------------------------\n",
    "data_dir          = Path(\"C:/Users/flobm/PRAMA_2025\")\n",
    "test_features_dir = data_dir / \"test_input/test_input/moco_features\"\n",
    "out_dir           = data_dir / \"Outputs\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1)  PARAMÈTRES INFERENCE\n",
    "# ------------------------------------------------------------------\n",
    "k_top_tiles = 200\n",
    "agg_method  = \"mean\"          # \"mean\" ou \"max\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2)  TRI “NATURAL SORT” + UTILS\n",
    "# ------------------------------------------------------------------\n",
    "def natural_key(path_obj):\n",
    "    \"\"\"Clé de tri type '10' > '2', gère aussi des préfixes/suffixes.\"\"\"\n",
    "    return [\n",
    "        int(tok) if tok.isdigit() else tok.lower()\n",
    "        for tok in re.split(r\"(\\d+)\", path_obj.name)\n",
    "    ]\n",
    "\n",
    "def sample_id_from_path(p: Path) -> str:\n",
    "    \"\"\"Récupère l'ID sans l'extension (si elle existe).\"\"\"\n",
    "    return p.stem if p.suffix else p.name\n",
    "# ------------------------------------------------------------------\n",
    "#  FONCTION PRINCIPALE  –  ENSEMBLE MOYENNÉ\n",
    "# ------------------------------------------------------------------\n",
    "def predict_test_and_save(\n",
    "    base_model,                # XGB entraîné sur toutes les tuiles (stage 1)\n",
    "    final_models,              # LISTE de modèles (stage 2 : 5 folds, par ex.)\n",
    "    k=200,                     # top-k tuiles conservées\n",
    "    aggregate=\"mean\",          # \"mean\" ou \"max\" tuile→lame\n",
    "    features_dir=test_features_dir,\n",
    "    output_dir=out_dir,\n",
    "    prefix=\"test_output\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Produit le CSV 'Sample ID,Target' avec probas moyennées sur plusieurs modèles.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "\n",
    "    files = sorted(\n",
    "        [p for p in features_dir.iterdir() if p.is_file()],\n",
    "        key=natural_key,\n",
    "    )\n",
    "\n",
    "    for path in files:\n",
    "        sid_raw = sample_id_from_path(path)      # ex. \"ID_022\"\n",
    "        sid_csv = f\"{sid_raw}.npy\"               # ex. \"ID_022.npy\"\n",
    "        mat     = np.load(path)[:, 3:]           # (n_tiles, 2048)\n",
    "\n",
    "        # --------- Étape A : scores base_model et top-k ---------\n",
    "        proba_base = base_model.predict_proba(mat)[:, 1]\n",
    "        if k is not None and len(proba_base) > k:\n",
    "            idx_keep = np.argsort(-proba_base)[:k]\n",
    "            feats    = mat[idx_keep]\n",
    "        else:\n",
    "            feats    = mat\n",
    "\n",
    "        # --------- Étape B : prédiction par CHAQUE modèle ---------\n",
    "        tile_preds = np.column_stack(\n",
    "            [m.predict_proba(feats)[:, 1] for m in final_models]\n",
    "        )                                           # shape (n_tiles, n_models)\n",
    "\n",
    "        # Moyenne des modèles pour chaque tuile\n",
    "        tile_mean = tile_preds.mean(axis=1)         # shape (n_tiles,)\n",
    "\n",
    "        # --------- Étape C : agrégation tuile → lame ---------\n",
    "        sample_prob = (\n",
    "            tile_mean.mean() if aggregate == \"mean\" else tile_mean.max()\n",
    "        )\n",
    "\n",
    "        preds.append((sid_csv, sample_prob))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #  EXPORT CSV\n",
    "    # ------------------------------------------------------------------\n",
    "    preds.sort(key=lambda x: natural_key(Path(x[0])))\n",
    "    ts       = datetime.datetime.now().strftime(\"%m_%d-%H_%M\")\n",
    "    out_path = output_dir / f\"{prefix}-{ts}.csv\"\n",
    "\n",
    "    pd.DataFrame(preds, columns=[\"Sample ID\", \"Target\"]).to_csv(out_path, index=False)\n",
    "    print(f\"[✓] Submission sauvegardée dans : {out_path}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  APPEL\n",
    "# ------------------------------------------------------------------\n",
    "#   • 'base'  : modèle entraîné stage 1 sur toutes les tuiles\n",
    "#   • 'models': liste contenant les 5 modèles “final” (stage 2, un par fold)\n",
    "predict_test_and_save(base, models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
