{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs and load dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Other imports\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "\n",
    "# folder = \"PRAMA2025/\"\n",
    "# modify_df = pd.read_csv(folder + 'train_with_clusters.csv')\n",
    "# test_df = pd.read_csv(folder + 'test_with_clusters.csv')\n",
    "\n",
    "# for those who have the datasets in the same folder as the python code (which is the way to go ngl)\n",
    "# florian is quite a flexer but we don't have to be like him\n",
    "\n",
    "origin_train_name = \"train_data\"\n",
    "origin_test_name = \"test_data\"\n",
    "\n",
    "train_df = pd.read_csv('CSV DATA/'+origin_train_name+'.csv')  # fichier csv VIERGE d'origine du train\n",
    "test_df = pd.read_csv('CSV DATA/'+origin_test_name+'.csv')  # fichier csv VIERGE d'origine du test\n",
    "\n",
    "#little printy print \n",
    "print(\"Train shape: \", train_df.shape)\n",
    "print(\"Test shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention nouvelle règle : \n",
    "\n",
    "- Damien is to come to the next classes with no underwear and a kilt.\n",
    "- We'll add a value between 0 and 1 showcasing the evolution of the year : 0 for the beginnin of the year, 1 for the end of the year. Then, we'll multiply this by pi and use the sine of this value.\n",
    "- This way we'll have values closer to 1 for 'colder' days and 0 for 'hotter' days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinus of all date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_ratios_for_date(modify_df, name):\n",
    "    # Preprocess the datasets\n",
    "    # Convert the Sales_Date to datetime format\n",
    "    modify_df['date'] = pd.to_datetime(modify_df['date'])\n",
    "\n",
    "    # Extract useful date features (e.g., year, month, day)\n",
    "    modify_df['Year'] = modify_df['date'].dt.year\n",
    "    modify_df['Month'] = modify_df['date'].dt.month\n",
    "    modify_df['Day'] = modify_df['date'].dt.day\n",
    "    modify_df['sin_month'] = modify_df['Month'].map(lambda x: np.sin(float(x)/12))\n",
    "\n",
    "    modify_df['day_of_year'] = modify_df['date'].dt.dayofyear\n",
    "    # Calcul du nombre de jours dans l'année (365 ou 366)\n",
    "    modify_df['days_in_year'] = np.where(modify_df['date'].dt.is_leap_year, 366, 365)\n",
    "    # Calcul El Famoso ratio (float entre 0 et 1)\n",
    "    modify_df['day_ratio'] = modify_df['day_of_year'] / modify_df['days_in_year']\n",
    "    # Calculer le sinus de pi * ratio\n",
    "    modify_df['sin_dayofyear'] = np.sin(np.pi * modify_df['day_ratio'])\n",
    "    modify_df.drop(columns=['day_of_year', 'days_in_year', 'day_ratio'], inplace=True)\n",
    "    modify_df = modify_df.drop(columns=['date', 'Year', 'Month', 'Day'])\n",
    "\n",
    "    new_name = name + \"_with_sin_ratios\"\n",
    "\n",
    "    return modify_df, new_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform df to have price/square meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_df_with_m2(to_modify_df, name):\n",
    "    # create a new data frame with everything plus a column corresponding to 1 if \"m2_jardin\" is >0, 0 otherwise\n",
    "    # we will also do the same for m2_etage and m2_soussol\n",
    "    modify_df = to_modify_df.copy()\n",
    "    modify_df['jardin'] = train_df['m2_jardin'].map(lambda x: 1 if x > 0 else 0)\n",
    "    modify_df['etage'] = train_df['m2_etage'].map(lambda x: 1 if x > 0 else 0)\n",
    "    modify_df['soussol'] = train_df['m2_soussol'].map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # now add three new columns per df, one for interior m2 and one for exterior m2 and one for total m2\n",
    "    # for interior we already have m2_interieur which is the sum of etage and soussol\n",
    "    modify_df['m2_outside'] = modify_df['m2_jardin']\n",
    "    modify_df['m2_total'] = modify_df['m2_interieur'] + modify_df['m2_outside']\n",
    "\n",
    "    # now we add two columns per df, one for prix per m2 interior and one for prix per m2 total\n",
    "    modify_df['prix_m2_interieur'] = modify_df['prix'] / modify_df['m2_interieur']\n",
    "    modify_df['prix_m2_total'] = modify_df['prix'] / modify_df['m2_total']\n",
    "\n",
    "    # now export\n",
    "    train_name = \"train_with_clusters_and_m2.csv\"\n",
    "    modify_df.to_csv(train_name, index=False)\n",
    "\n",
    "    print(\"Train shape: \", modify_df.shape)\n",
    "\n",
    "    new_name = name + \"_with_m2_price\"\n",
    "\n",
    "    return modify_df, new_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(df, name, scaler = None):\n",
    "    if scaler :\n",
    "        scaler = scaler\n",
    "    else :\n",
    "        # Initialize the MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale the data, but not the ID column\n",
    "    columns_to_scale = df.columns.difference(['id', 'lat', 'long', 'sin_month', 'sin_dayofyear'])\n",
    "    to_scale_df1 = df[columns_to_scale].copy() # to not modify the original one\n",
    "\n",
    "    scaled_df1 = df.copy()\n",
    "    scaled_df1[columns_to_scale] = scaler.fit_transform(to_scale_df1)\n",
    "\n",
    "    scaled_name = name + \"_scaled\"\n",
    "\n",
    "    return scaler, scaled_df1, scaled_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exporting_df(df, name):\n",
    "    df.to_csv('CSV data/'+ name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the transfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates to sinus function\n",
    "train_sin, train_sin_name = sin_ratios_for_date(train_df, origin_train_name)\n",
    "test_sin, test_sin_name = sin_ratios_for_date(test_df, origin_test_name)\n",
    "\n",
    "# find price per square meter\n",
    "train_sin_m2, train_sin_m2_name = create_new_df_with_m2(train_sin, train_sin_name)\n",
    "\n",
    "# Scale the data\n",
    "scaler1, train_sin_m2_scaled, train_sin_m2_scaled_name = scaling_data(train_sin_m2, train_sin_m2_name)\n",
    "scaler1, train_sin_scaled, train_sin_scaled_name = scaling_data(train_sin, train_sin_name, scaler=scaler1)\n",
    "scaler1, test_sin_scaled, test_sin_scaled_name = scaling_data(test_sin, test_sin_name, scaler=scaler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sin_m2_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporting_df(train_sin_m2_scaled, train_sin_m2_scaled_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = train_test_split(train_new.drop(columns=['prix']), train_new['prix'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = ['cluster_tres_bas', 'cluster_bas', 'cluster_moyen', 'cluster_eleve', 'cluster_tres_eleve']\n",
    "# train_new.drop(columns=clusters, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
